env:
    SEED: 42
    CUDA_DEVICE_ORDER: "PCI_BUS_ID"
    OMP_NUM_THREADS: "4"
    # TRANSFORMERS_CACHE: /home/data/hf_cache/
paths:
    data_dir: "data/"
    log_dir: "artifacts/logs/"
    checkpoint_dir: "artifacts/checkpoints/"
    results_dir: "artifacts/results/"
model:
    # path: "google/gemma-3-4b-it"
    # path: Qwen/Qwen2.5-7B-Instruct
    # path: google/gemma-3-4b-it
    # path: NousResearch/Llama-2-7b-chat-hf
    # path: Qwen/Qwen2.5-7B-Instruct
    # path: google/gemma-3-4b-it
    path: meta-llama/Llama-2-7b
    seqlen: 2048
benchmarks:
    ppl_wikitext2:
        run_ppl: True
        batch_size: 8
    harness:
        run_lm_eval: True
        tasks:
            [
                "boolq",
                "winogrande",
                "piqa",
                "arc_easy"
            ]
        num_fewshot: 0
        batch_size: 1024
        apply_chat_template: False # for instructive models run with True and False
pruning:
    sparsity_type: semi-structured_act_magnitude # unstructured_act_magnitude, unstructured_weight_magnitude
    transformation_type: learnable # none
    sparsity_ratio: 0.2 # for unstructured pruning
    additional_transformation: "scaling"
    prune_n: 8
    prune_m: 16
    module: layers #layers mlp_blocks attn_blocks
    target_modules:
        [
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ]

